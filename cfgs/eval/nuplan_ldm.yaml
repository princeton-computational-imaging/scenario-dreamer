defaults:
- base

run_name: scenario_dreamer_ldm_base_nuplan # default run name for evaluation

mode: initial_scene # or lane_conditioned or inpainting or metrics or simulation_environments
num_samples: 100 # number of samples to generate during evaluation
init_prob_matrix_path: ${project_root}/metadata/initial_prob_matrix_nuplan.pt # path to initial scene num lanes and agents probability matrix
inpainting_prob_matrix_path: ${project_root}/metadata/inpainting_prob_matrix_nuplan.pt # path to inpainting num lanes and agents probability matrix
batch_size: 32 # batch size for evaluation
visualize: False # visualize samples during evaluation?
viz_dir: ${project_root}/viz_gen_samples_${ldm.eval.run_name} # directory to save visualizations of generated samples
cache_samples: False # cache samples to disk?
conditioning_path: null # optional path to conditioning data for lane-conditioned or inpainting evaluation

sim_envs:
  num_inpainting_candidates: 16 # number of inpainting candidates to generate per partial scene when generating simulation environments
  overhead_factor: 20 # generate num_samples * overhead_factor initial scenes to account for filtering when generating simulation environments
  route_length: 500 # length of routes (in meters) to generate simulation environments for

metrics:
  samples_path: ${dataset_root}/checkpoints/${ldm.eval.run_name}/initial_scene_samples # path to load generated samples
  metrics_save_path: ${dataset_root}/checkpoints/${ldm.eval.run_name} # path to save metrics
  eval_set: ${project_root}/metadata/nuplan_eval_set.pkl # pickle containing paths to ground-truth samples for metrics computation
  gt_agent_test_dir: ${dataset_root}/scenario_dreamer_ae_preprocess_nuplan/test # directory containing ground-truth agent data for metrics computation
  gt_lane_test_dir: ${dataset_root}/scenario_dreamer_nuplan/sledge_raw/test # directory containing ground-truth lane data for metrics computation